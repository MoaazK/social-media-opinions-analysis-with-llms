{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8396f86f-873e-41e4-927e-a237d57113d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    pairwise,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "from utils.data_preprocessing import preprocess_data, split_data, split_data_all\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d6e3648-58cc-4180-bedf-09d95f9bd406",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, opinions, conclusions = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f72fd4-656a-4a88-965d-be8251da5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = topics.merge(conclusions[['topic_id', 'text']], on='topic_id', suffixes=('_topic', '_conclusion'))\n",
    "\n",
    "# Group opinions by 'topic_id' and concatenate their texts\n",
    "# opinions_grouped = opinions.groupby('topic_id')['text'].apply(lambda texts: ' '.join(texts)).reset_index()\n",
    "opinions_grouped = opinions.groupby('topic_id')['text'].apply(lambda texts: '\\n'.join(text.strip() for text in texts)).reset_index()\n",
    "\n",
    "data = data.merge(opinions_grouped, on='topic_id', how='left')\n",
    "data.rename(columns={'text_topic': 'topic_text', 'text_conclusion': 'target_text', 'text': 'opinions_text'}, inplace=True)\n",
    "\n",
    "data['opinions_text'] = data['opinions_text'].fillna('')\n",
    "\n",
    "data['input_text'] = 'summarize: ' + data['topic_text'] + ' ' + data['opinions_text']\n",
    "\n",
    "data = data[['input_text', 'target_text']]\n",
    "data = data.dropna(subset=['input_text', 'target_text'])\n",
    "data = data[data['input_text'].str.strip() != '']\n",
    "data = data[data['target_text'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a3f257-806a-450f-8e39-d0f045acc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8642e8c3-24ba-473c-9f74-44fe1cba64fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f222605-af45-49be-a71d-cb8c86312330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name = 't5-small'\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ac62d7-8fb6-4d9f-bbc1-162c2e8edbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3dbac2e-0d06-42b2-810b-c0708fa38921",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 150\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['input_text']\n",
    "    targets = examples['target_text']\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22a82133-af59-475c-a6aa-6b4496608980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bde58edd00647b6849b177dd0bb4033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2656 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/newmind/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4117: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08b5eaca06f4629841cbb7e10e848bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/664 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the datasets\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebc16ce5-850d-402b-abc0-60dee279613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load('rouge')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    # print(result)\n",
    "    # print(result.items())\n",
    "\n",
    "    # result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result['gen_len'] = np.mean(prediction_lens)\n",
    "\n",
    "    # beam_predictions = model.generate(\n",
    "    #     input_ids,\n",
    "    #     max_length=max_target_length,\n",
    "    #     num_beams=5,\n",
    "    #     early_stopping=True\n",
    "    # )\n",
    "\n",
    "    # result['gen_len'] = np.mean([len(pred) for pred in beam_predictions])\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e91bb913-a1e9-4ca0-9a12-2adcc3030595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='664' max='664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [664/664 19:38, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>10.306800</td>\n",
       "      <td>7.155865</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.082700</td>\n",
       "      <td>0.193200</td>\n",
       "      <td>0.193200</td>\n",
       "      <td>38.959300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.622300</td>\n",
       "      <td>1.755919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.855300</td>\n",
       "      <td>1.645716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.721600</td>\n",
       "      <td>1.591260</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.058700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.761200</td>\n",
       "      <td>1.558850</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>7.825300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.611500</td>\n",
       "      <td>1.544973</td>\n",
       "      <td>0.161800</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>0.114800</td>\n",
       "      <td>0.115400</td>\n",
       "      <td>18.994000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.689800</td>\n",
       "      <td>1.531600</td>\n",
       "      <td>0.284600</td>\n",
       "      <td>0.099300</td>\n",
       "      <td>0.199100</td>\n",
       "      <td>0.199600</td>\n",
       "      <td>34.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.542500</td>\n",
       "      <td>1.528818</td>\n",
       "      <td>0.290500</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.203700</td>\n",
       "      <td>0.204200</td>\n",
       "      <td>35.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.624900</td>\n",
       "      <td>1.523454</td>\n",
       "      <td>0.299000</td>\n",
       "      <td>0.103700</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.210400</td>\n",
       "      <td>36.600900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.623200</td>\n",
       "      <td>1.521338</td>\n",
       "      <td>0.308400</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>0.216900</td>\n",
       "      <td>0.217200</td>\n",
       "      <td>37.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.591600</td>\n",
       "      <td>1.519163</td>\n",
       "      <td>0.309100</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.217700</td>\n",
       "      <td>37.701800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.621500</td>\n",
       "      <td>1.517884</td>\n",
       "      <td>0.308400</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.217800</td>\n",
       "      <td>37.563300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.650300</td>\n",
       "      <td>1.517348</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.108900</td>\n",
       "      <td>0.218200</td>\n",
       "      <td>0.218500</td>\n",
       "      <td>37.637000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=664, training_loss=2.460550210562097, metrics={'train_runtime': 1179.3869, 'train_samples_per_second': 4.504, 'train_steps_per_second': 0.563, 'total_flos': 718935649419264.0, 'train_loss': 2.460550210562097, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./saved_models/summarization',\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    # gradient_accumulation_steps=2,\n",
    "    num_train_epochs=2,\n",
    "    save_steps=300,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    # weight_decay=0.001,\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_total_limit=0,\n",
    "    # save_strategy=\"no\",\n",
    "    load_best_model_at_end=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=40,\n",
    "    generation_num_beams=5,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18d5c670-ee79-4437-9f2d-f8f5701a9cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 01:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5178840160369873, 'eval_rouge1': 0.3084, 'eval_rouge2': 0.1081, 'eval_rougeL': 0.2175, 'eval_rougeLsum': 0.2178, 'eval_gen_len': 37.5633, 'eval_runtime': 73.1439, 'eval_samples_per_second': 9.078, 'eval_steps_per_second': 0.574, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1db9606-e4c8-42af-87ec-42aa2f9f3880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('saved_models/summarization/tokenizer_config.json',\n",
       " 'saved_models/summarization/special_tokens_map.json',\n",
       " 'saved_models/summarization/spiece.model',\n",
       " 'saved_models/summarization/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model('saved_models/summarization/')\n",
    "tokenizer.save_pretrained('saved_models/summarization/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b74749f-0a53-4f73-a627-8a8bb4262fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Conclusion:\n",
      " with so many things in this world that few people agree on this is a nice change to see in regards the removal of so many cars that few people agree on this is a nice change to see in regards to the removal of so many cars\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('saved_models/summarization')\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def summarize(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    input_ids = input_ids\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=150,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "topic_id_example = topics['text'].iloc[1]\n",
    "# input_text = generate_input_text(topic_id_example)\n",
    "input_text = 'summarize: ' + topic_id_example\n",
    "if input_text:\n",
    "    conclusion = summarize(input_text)\n",
    "    print(\"Generated Conclusion:\\n\", conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e444dc7d-f556-45f3-9ca7-41d509ace4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Conclusion:\n",
      " there are many sides to people giving up their cars some people are truly happy and some are not it may not be that bad i mean how did people manige before cars were even invented\n"
     ]
    }
   ],
   "source": [
    "topic_id_example = topics['text'].iloc[8]\n",
    "# input_text = generate_input_text(topic_id_example)\n",
    "input_text = 'summarize: ' + topic_id_example\n",
    "if input_text:\n",
    "    conclusion = summarize(input_text)\n",
    "    print(\"Generated Conclusion:\\n\", conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537196a-57d0-4f06-bda5-4c2ee1b46345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newmind",
   "language": "python",
   "name": "newmind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
